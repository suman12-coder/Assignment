{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SVM and Naive Bayes : Assignment\n"
      ],
      "metadata": {
        "id": "PTh5iJCZHZaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "Answer:\n",
        "\n",
        "SVM is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that separates data points of different classes with the maximum margin. The data points closest to the hyperplane are called support vectors, and they determine the position and orientation of the hyperplane.\n",
        "\n",
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "Answer:\n",
        "- Hard Margin SVM assumes data is perfectly linearly separable and does not allow misclassification. It maximizes the margin strictly.\n",
        "- Soft Margin SVM allows some misclassification to handle noisy or overlapping data. It introduces a regularization parameter (C) to balance margin maximization and classification error.\n",
        "\n",
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "\n",
        "Answer:\n",
        "The Kernel Trick allows SVM to operate in high-dimensional spaces without explicitly computing the coordinates. It transforms non-linearly separable data into a higher-dimensional space where it becomes linearly separable.\n",
        "\n",
        "Example:\n",
        "- RBF Kernel (Radial Basis Function): Useful when the decision boundary is non-linear. It maps data into infinite-dimensional space using Gaussian functions.\n",
        "\n",
        "Question 4: What is a Naive Bayes Classifier, and why is it called \"naive\"?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Naive Bayes is a probabilistic classifier based on Bayes’ Theorem. It assumes that features are conditionally independent given the class label—this assumption is \"naive\" because it rarely holds true in real-world data.\n",
        "\n",
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naive Bayes variants. When would you use each one?\n",
        "\n",
        "Answer:\n",
        "- Gaussian NB: For continuous features (e.g., height, weight). Assumes features follow a normal distribution.\n",
        "- Multinomial NB: For discrete count data (e.g., word frequencies in text).\n",
        "- Bernoulli NB: For binary features (e.g., presence or absence of words).\n"
      ],
      "metadata": {
        "id": "6UO7sikPFXNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6:   Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "hVEKwZ7uFXKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "#loading the dataset:\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "x= iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size = .30,random_state = 42)\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "#Results\n",
        "print(f\"Accuracy : {accuracy_score(y_test,y_pred)}\\n\")\n",
        "print(f\"Support Vectors are :\\n {clf.support_vectors_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj5n08sXKfeX",
        "outputId": "b3f97356-d50e-4800-f69d-a51388ba2d92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 1.0\n",
            "\n",
            "Support Vectors are :\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7:  Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "Answer:  "
      ],
      "metadata": {
        "id": "IXO47ymqFXHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# loading the dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "dataset = load_breast_cancer()\n",
        "x = dataset.data\n",
        "y = dataset.target\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size = .30,random_state = 42)\n",
        "model = GaussianNB()\n",
        "model.fit(X_train,y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "# results\n",
        "print(f'Classification report :\\n {classification_report(y_test,y_pred)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdbQJQe3N2Ma",
        "outputId": "15e0e651-6f26-4434-b281-68a480045db0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report :\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.90      0.92        63\n",
            "           1       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "d_ocvGNhFXD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "dataset = load_wine()\n",
        "x = dataset.data\n",
        "y = dataset.target\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size = .30,random_state = 42)\n",
        "\n",
        "#hyperparameter tuning\n",
        "params ={\n",
        "    'C':[1,2,3],\n",
        "    'gamma':[.1,.2,.3],\n",
        "    'kernel':['linear','rbf']\n",
        "}\n",
        "model = GridSearchCV(SVC(),param_grid= params,cv=3,verbose = 2,n_jobs=-1)\n",
        "model.fit(X_train,y_train)\n",
        "y_pred = model.best_estimator_.predict(X_test)\n",
        "\n",
        "# Results\n",
        "print(f'Accuracy: {accuracy_score(y_test,y_pred):.3f}')\n",
        "print(f'Hyparparameters : {model.best_params_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ1UHTdJPtvy",
        "outputId": "6229bfaf-2be0-4f43-e765-fd14f843e5a0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
            "Accuracy: 0.981\n",
            "Hyparparameters : {'C': 1, 'gamma': 0.1, 'kernel': 'linear'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "WuV6kSsrFW7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# 1. Load dataset (subset for speed)\n",
        "categories = ['rec.sport.baseball', 'sci.space', 'talk.politics.mideast']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# 4. Train a Naïve Bayes Classifier\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# 5. Get prediction probabilities\n",
        "y_prob = model.predict_proba(X_test_tfidf)\n",
        "\n",
        "# 6. Calculate ROC-AUC Score (multi-class)\n",
        "roc_auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
        "\n",
        "# 7. Print result\n",
        "print(f\"ROC-AUC Score of Naive Bayes Classifier:{roc_auc:.5f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAgPlvlwaMn7",
        "outputId": "d9e6f3c0-a792-41f3-aacc-bedc59aee3db"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score of Naive Bayes Classifier:0.99328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "Ja0wU3JTFW3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Preprocessing:\n",
        "  - Use TfidfVectorizer for text vectorization.\n",
        "  - Handle missing data using imputation or removal.\n",
        "- Model Choice:\n",
        "  - Naïve Bayes(MultinomialNB) is preferred for text due to speed and performance on sparse data.\n",
        "  - SVM Works well with high-dimensional text data & Can find non-linear boundaries with kernels.\n",
        "  - For a production spam classifier, I would start with SVM with linear kernel because:\n",
        "  1. It handles high-dimensional text data effectively.\n",
        "  2. It usually achieves higher accuracy than Naïve Bayes in real-world spam filtering tasks.\n",
        "- Class Imbalance:\n",
        "  - Use SMOTE or class weights.\n",
        "- Evaluation Metrics:\n",
        "  - Precision, Recall, F1-score, ROC-AUC, Confusion Matrix.\n",
        "- Business Impact:\n",
        "  - Reduces manual filtering, improves productivity, and protects users from phishing.\n"
      ],
      "metadata": {
        "id": "dvYta-Y-c97G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# SPAM vs NOT SPAM CLASSIFIER\n",
        "\n",
        "\n",
        "# 1. Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# 2. Create or load a sample dataset (simulate)\n",
        "data = {\n",
        "    'text': [\n",
        "        'Win a free iPhone now!!!',\n",
        "        'Meeting at 10am with the sales team',\n",
        "        'Congratulations, you have won a lottery prize!',\n",
        "        'Lunch with client tomorrow',\n",
        "        'Get cheap medicines online',\n",
        "        None,  # missing data example\n",
        "        'Important update about your account',\n",
        "        'Earn money from home easily',\n",
        "        'Schedule weekly report discussion',\n",
        "        'Claim your free vacation tickets now'\n",
        "    ],\n",
        "    'label': ['spam','ham','spam','ham','spam','ham','ham','spam','ham','spam']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 3. Handle missing data\n",
        "df['text'] = df['text'].fillna(\"missing_text\")\n",
        "\n",
        "# 4. Encode labels (spam = 1, ham = 0)\n",
        "df['label'] = df['label'].map({'ham':0, 'spam':1})\n",
        "\n",
        "# 5. Split into training and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['text'], df['label'], test_size=0.3, random_state=42, stratify=df['label']\n",
        ")\n",
        "\n",
        "# 6. Vectorize text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# 7. Handle class imbalance by adjusting class weights in SVM\n",
        "model = LinearSVC(class_weight='balanced', random_state=42)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# 8. Make predictions\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# 9. Evaluate the model\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# For ROC-AUC, we need decision scores (since LinearSVC doesn't give probabilities)\n",
        "y_scores = model.decision_function(X_test_tfidf)\n",
        "roc_auc = roc_auc_score(y_test, y_scores)\n",
        "print(\"ROC-AUC Score:\", round(roc_auc, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBZqpMBMbxQM",
        "outputId": "c0913248-dce8-44ea-f766-bb47e0bbfc73"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         2\n",
            "           1       0.33      1.00      0.50         1\n",
            "\n",
            "    accuracy                           0.33         3\n",
            "   macro avg       0.17      0.50      0.25         3\n",
            "weighted avg       0.11      0.33      0.17         3\n",
            "\n",
            "Confusion Matrix:\n",
            " [[0 2]\n",
            " [0 1]]\n",
            "ROC-AUC Score: 1.0\n"
          ]
        }
      ]
    }
  ]
}